{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae620cda-ae0b-44f0-8d52-289e874ae1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.19.1+cu118)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.8/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.8/dist-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.8/dist-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.8/dist-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.8/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.8/dist-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1866aa2-d537-412d-ae26-f39e8e7334e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e2880a-3c3a-46fb-b572-e23f50351ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "GPU Name: NVIDIA H100 PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16685bb6-8d16-44ce-87cb-3c70e55483bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA H100 PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46fc30d-29fc-473e-a03b-7150c7bb41bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0301fea06d43d1afd06bd0fa375627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce4aad4e31c4303bf5c2c750fe0b1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24ef99771a1427abde3807962eeda61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0decf5b339543a68fd87d963f6bec60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e566ac1f6704d9a83e353a9cd5fa799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8bcc45840e43b0a45d2033063ade46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/1.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c7f087e6294569a5c019729eba825e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/1.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a654f3abb64988bb8a33aeb8f54e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d369f1c9a54a099e6d9d925355cf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6860d315a665453d98b178a7a9c42739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/262M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c94d5cdf86345278a3e8c163a308444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b54302b44647abbbda57263f91ca9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: What is pneumonia?\n",
      "Pneumonia is an infection of the lungs. It is caused by bacteria, viruses, or fungi. The infection can spread to the lungs through the air or through the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Hugging Face repo for Meditron-7b\n",
    "model_name = \"epfl-llm/meditron-7b\"\n",
    "token = \"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\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=token,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Example input\n",
    "input_text = \"What is pneumonia?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=50)\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df6c8433-f4c6-4997-8536-70706c1b2dda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6156a99f-5cf5-4575-b2ea-3388235c5bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/2c/92/48aec3736ca778ffe5fa68e19e3c18917cba4de43fa46fe6176cccafe267/accelerate-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (22.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (2.4.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44858eb-56df-4c5a-bcf0-cefd775f5677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bb364c90c8429bb5539b5deb20dcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b14542cb824f65902ddb68605eb878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5421740782b8446e8c2746f064ed5f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921eb32db6ff44a9bf6b6c2aa0c10641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/344 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f50ad24cbd4ce7bec8d8a23a1f8113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2ad024e4c84906b89bfac6008ed6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8baa629e2c7148508637b5e445e06e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/59.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e209946a7f499ca05b126e9080289f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f025457ab0ef4b2bb176a57fd8edb661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/19.7G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf5ea94be72464db28cc827262a6bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/19.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f22a9c73e34f4ca78913d5244d3b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/19.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b9bba136234ba183d9954fb4b73958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/19.6G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5350515b1954ed888fdec336e8df29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/19.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de40a277c6c44f3a99711df524de3845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/19.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5520f1931d6496998fd5399ab6cad32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/19.6G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9909245185a4bb1870e2ac3bb125705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a7c89c218946ca8e4d415605461e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2a9cd8e10b495c849b16406c615ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: What is pneumonia?\n",
      "Pneumonia is an infection of the lungs. It can be caused by bacteria, viruses, fungi, or parasites. Bacteria are the most common cause of p\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Hugging Face repo for Meditron-7b\n",
    "model_name = \"epfl-llm/meditron-70b\"\n",
    "token = \"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\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=token,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Example input\n",
    "input_text = \"What is pneumonia?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=50)\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e6a879-dc85-4c92-a9cb-9ca9c24babff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34461ceb63ef4c42bf20dbc3f53bfeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=token,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"  # Spreads the model across multiple GPUs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb9df1-7f82-4daa-9eaf-41083ece80f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available. Details:\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706ed03-bae5-49e8-9236-cf2e65c88846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor and move it to GPU\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0]).cuda()\n",
    "print(\"Tensor on GPU:\", tensor)\n",
    "\n",
    "# Perform a calculation on the GPU\n",
    "result = tensor * 2\n",
    "print(\"Result of GPU computation:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59467fe7-5f01-4f0d-94f0-5797656ceaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_query(model, tokenizer, query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            pad_token_id=tokenizer.eos_token_id  # Explicitly set pad_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response, end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e2aa49-c68f-4cc8-b744-df136f8abd26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODELS[model_name], token=HF_TOKEN)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad_token_id globally\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODELS[model_name],\n",
    "        token=HF_TOKEN,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb61a6d-c12a-4ce7-864d-d2c824f0d82d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39meos_token_id\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea6491c-f03d-470a-b110-e0860f2093c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define model name and token\n",
    "model_name = \"epfl-llm/meditron-7b\"\n",
    "token = \"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\"  # Replace with your actual Hugging Face token\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "\n",
    "# Fix the pad_token_id issue\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247d5128-e0e2-4962-b086-75969fb395ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c3edb4fe334b78a8ea0e38ba5f2b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 1 concurrent users...\n",
      "Average Response Time (1 users): 1.03s\n",
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c61c189c584f5cab40829a248e7f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 2 concurrent users...\n",
      "Average Response Time (2 users): 2.02s\n",
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e98eca6d044207b663bb33364f71e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 4 concurrent users...\n",
      "Average Response Time (4 users): 4.45s\n",
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf895b53e144aacb8b36b0429fd9c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 8 concurrent users...\n",
      "Average Response Time (8 users): 9.17s\n",
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16aadcbc72ba49399c43823086b48917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 16 concurrent users...\n",
      "Average Response Time (16 users): 20.02s\n",
      "Results saved to results_meditron_7b.txt.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"Meditron-7B\"\n",
    "MODEL_PATH = \"epfl-llm/meditron-7b\"\n",
    "TOKEN = \"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\"\n",
    "\n",
    "# Load the model\n",
    "def load_model():\n",
    "    print(f\"Loading {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=TOKEN)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        token=TOKEN,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    # Fix: Set pad_token to eos_token if it is not already defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def process_query(model, tokenizer, query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            pad_token_id=tokenizer.pad_token_id  # Explicitly set pad_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True), duration\n",
    "\n",
    "\n",
    "# Function to simulate concurrent users\n",
    "def run_trials(queries, concurrent_users=1):\n",
    "    tokenizer, model = load_model()\n",
    "    print(f\"Running {MODEL_NAME} with {concurrent_users} concurrent users...\")\n",
    "\n",
    "    def run_single_query(query):\n",
    "        _, duration = process_query(model, tokenizer, query)\n",
    "        return duration\n",
    "\n",
    "    times = []\n",
    "    with ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "        results = executor.map(run_single_query, queries)\n",
    "        times.extend(results)\n",
    "\n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"Average Response Time ({concurrent_users} users): {avg_time:.2f}s\")\n",
    "    return avg_time\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    queries = [\"What is pneumonia?\", \"Explain flu symptoms.\", \"How to treat COVID-19?\"] * 10  # Test load\n",
    "    results = {}\n",
    "\n",
    "    for load in [1, 2, 4, 8, 16]:\n",
    "        avg_time = run_trials(queries, concurrent_users=load)\n",
    "        results[load] = avg_time\n",
    "\n",
    "    # Save results\n",
    "    with open(\"results_meditron_7b.txt\", \"w\") as f:\n",
    "        f.write(str(results))\n",
    "    print(\"Results saved to results_meditron_7b.txt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563badf-e685-4f41-909a-ea8376dbc87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"Meditron-70B\"\n",
    "MODEL_PATH = \"epfl-llm/meditron-70b\"\n",
    "TOKEN = \"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\"\n",
    "\n",
    "# Load the model\n",
    "def load_model():\n",
    "    print(f\"Loading {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=TOKEN)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        token=TOKEN,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    # Fix: Set pad_token to eos_token if it is not already defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# Function to process a single query\n",
    "def process_query(model, tokenizer, query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            pad_token_id=tokenizer.pad_token_id  # Explicitly set pad_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True), duration\n",
    "\n",
    "\n",
    "# Function to simulate concurrent users\n",
    "def run_trials(queries, concurrent_users=1):\n",
    "    tokenizer, model = load_model()\n",
    "    print(f\"Running {MODEL_NAME} with {concurrent_users} concurrent users...\")\n",
    "\n",
    "    def run_single_query(query):\n",
    "        _, duration = process_query(model, tokenizer, query)\n",
    "        return duration\n",
    "\n",
    "    times = []\n",
    "    with ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "        results = executor.map(run_single_query, queries)\n",
    "        times.extend(results)\n",
    "\n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"Average Response Time ({concurrent_users} users): {avg_time:.2f}s\")\n",
    "    return avg_time\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    queries = [\"What is pneumonia?\", \"Explain flu symptoms.\", \"How to treat COVID-19?\"] * 10  # Test load\n",
    "    results = {}\n",
    "\n",
    "    for load in [1, 2, 4, 8, 16]:\n",
    "        avg_time = run_trials(queries, concurrent_users=load)\n",
    "        results[load] = avg_time\n",
    "\n",
    "    # Save results\n",
    "    with open(\"results_meditron_70b.txt\", \"w\") as f:\n",
    "        f.write(str(results))\n",
    "    print(\"Results saved to results_meditron_70b.txt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "##################################### Next step ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c43b7e8-a5b7-4278-a3b5-5dfa67e13659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA H100 PCIe\n",
      "Tensor successfully created on GPU 0: tensor([-2.5251], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpus():\n",
    "    # Check total number of GPUs\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs available: {num_gpus}\")\n",
    "        \n",
    "        # Loop through each GPU and print its details\n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "        \n",
    "        # Verify if tensors can be sent to all GPUs\n",
    "        try:\n",
    "            for i in range(num_gpus):\n",
    "                device = torch.device(f\"cuda:{i}\")\n",
    "                tensor = torch.randn(1).to(device)  # Create a tensor on each GPU\n",
    "                print(f\"Tensor successfully created on GPU {i}: {tensor}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error when accessing GPU {i}: {e}\")\n",
    "    else:\n",
    "        print(\"No GPU detected. Using CPU.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf19a6f-8e2c-4757-b2f3-6fe7ac172975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b89887643944eaba2842599d0251d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Batch Size: 1, Sequence Length: 128...\n",
      "Success: Batch Size 1, Sequence Length 128\n",
      "Testing Batch Size: 1, Sequence Length: 256...\n",
      "Success: Batch Size 1, Sequence Length 256\n",
      "Testing Batch Size: 1, Sequence Length: 512...\n",
      "Success: Batch Size 1, Sequence Length 512\n",
      "Testing Batch Size: 1, Sequence Length: 1024...\n",
      "Success: Batch Size 1, Sequence Length 1024\n",
      "Testing Batch Size: 1, Sequence Length: 2048...\n",
      "Success: Batch Size 1, Sequence Length 2048\n",
      "Testing Batch Size: 2, Sequence Length: 128...\n",
      "Success: Batch Size 2, Sequence Length 128\n",
      "Testing Batch Size: 2, Sequence Length: 256...\n",
      "Success: Batch Size 2, Sequence Length 256\n",
      "Testing Batch Size: 2, Sequence Length: 512...\n",
      "Success: Batch Size 2, Sequence Length 512\n",
      "Testing Batch Size: 2, Sequence Length: 1024...\n",
      "Success: Batch Size 2, Sequence Length 1024\n",
      "Testing Batch Size: 2, Sequence Length: 2048...\n",
      "Success: Batch Size 2, Sequence Length 2048\n",
      "Testing Batch Size: 4, Sequence Length: 128...\n",
      "Success: Batch Size 4, Sequence Length 128\n",
      "Testing Batch Size: 4, Sequence Length: 256...\n",
      "Success: Batch Size 4, Sequence Length 256\n",
      "Testing Batch Size: 4, Sequence Length: 512...\n",
      "Success: Batch Size 4, Sequence Length 512\n",
      "Testing Batch Size: 4, Sequence Length: 1024...\n",
      "Success: Batch Size 4, Sequence Length 1024\n",
      "Testing Batch Size: 4, Sequence Length: 2048...\n",
      "Success: Batch Size 4, Sequence Length 2048\n",
      "Testing Batch Size: 8, Sequence Length: 128...\n",
      "Success: Batch Size 8, Sequence Length 128\n",
      "Testing Batch Size: 8, Sequence Length: 256...\n",
      "Success: Batch Size 8, Sequence Length 256\n",
      "Testing Batch Size: 8, Sequence Length: 512...\n",
      "Success: Batch Size 8, Sequence Length 512\n",
      "Testing Batch Size: 8, Sequence Length: 1024...\n",
      "Success: Batch Size 8, Sequence Length 1024\n",
      "Testing Batch Size: 8, Sequence Length: 2048...\n",
      "Success: Batch Size 8, Sequence Length 2048\n",
      "Testing Batch Size: 16, Sequence Length: 128...\n",
      "Success: Batch Size 16, Sequence Length 128\n",
      "Testing Batch Size: 16, Sequence Length: 256...\n",
      "Success: Batch Size 16, Sequence Length 256\n",
      "Testing Batch Size: 16, Sequence Length: 512...\n",
      "Success: Batch Size 16, Sequence Length 512\n",
      "Testing Batch Size: 16, Sequence Length: 1024...\n",
      "Success: Batch Size 16, Sequence Length 1024\n",
      "Testing Batch Size: 16, Sequence Length: 2048...\n",
      "Success: Batch Size 16, Sequence Length 2048\n",
      "\n",
      "Test Results:\n",
      "Batch Size: 1, Sequence Length: 128 => Pass\n",
      "Batch Size: 1, Sequence Length: 256 => Pass\n",
      "Batch Size: 1, Sequence Length: 512 => Pass\n",
      "Batch Size: 1, Sequence Length: 1024 => Pass\n",
      "Batch Size: 1, Sequence Length: 2048 => Pass\n",
      "Batch Size: 2, Sequence Length: 128 => Pass\n",
      "Batch Size: 2, Sequence Length: 256 => Pass\n",
      "Batch Size: 2, Sequence Length: 512 => Pass\n",
      "Batch Size: 2, Sequence Length: 1024 => Pass\n",
      "Batch Size: 2, Sequence Length: 2048 => Pass\n",
      "Batch Size: 4, Sequence Length: 128 => Pass\n",
      "Batch Size: 4, Sequence Length: 256 => Pass\n",
      "Batch Size: 4, Sequence Length: 512 => Pass\n",
      "Batch Size: 4, Sequence Length: 1024 => Pass\n",
      "Batch Size: 4, Sequence Length: 2048 => Pass\n",
      "Batch Size: 8, Sequence Length: 128 => Pass\n",
      "Batch Size: 8, Sequence Length: 256 => Pass\n",
      "Batch Size: 8, Sequence Length: 512 => Pass\n",
      "Batch Size: 8, Sequence Length: 1024 => Pass\n",
      "Batch Size: 8, Sequence Length: 2048 => Pass\n",
      "Batch Size: 16, Sequence Length: 128 => Pass\n",
      "Batch Size: 16, Sequence Length: 256 => Pass\n",
      "Batch Size: 16, Sequence Length: 512 => Pass\n",
      "Batch Size: 16, Sequence Length: 1024 => Pass\n",
      "Batch Size: 16, Sequence Length: 2048 => Pass\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Model and Token Configuration\n",
    "MODEL_PATH = \"epfl-llm/meditron-7b\"\n",
    "TOKEN = \"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\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "def load_model():\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=TOKEN)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        token=TOKEN,\n",
    "        torch_dtype=torch.float16,  # Mixed precision\n",
    "        device_map=\"auto\"           # Automatically allocate to GPU\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to Test Memory for Given Batch Size and Sequence Length\n",
    "def test_hyperparameters(model, tokenizer, batch_size, seq_length):\n",
    "    try:\n",
    "        print(f\"Testing Batch Size: {batch_size}, Sequence Length: {seq_length}...\")\n",
    "        input_text = \"Test input sentence. \" * (seq_length // 5)  # Simulate input text\n",
    "        inputs = tokenizer(\n",
    "            [input_text] * batch_size,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=seq_length\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Perform a forward pass with max_new_tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,  # Replace max_length with max_new_tokens\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        print(f\"Success: Batch Size {batch_size}, Sequence Length {seq_length}\")\n",
    "        return True\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"OOM Error: Batch Size {batch_size}, Sequence Length {seq_length}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Main Function to Test Combinations\n",
    "def main():\n",
    "    tokenizer, model = load_model()\n",
    "\n",
    "    # Range of Hyperparameters to Test\n",
    "    batch_sizes = [1, 2, 4, 8, 16]          # Micro batch sizes\n",
    "    sequence_lengths = [128, 256, 512, 1024, 2048]  # Sequence lengths\n",
    "\n",
    "    results = []\n",
    "    for batch_size in batch_sizes:\n",
    "        for seq_length in sequence_lengths:\n",
    "            success = test_hyperparameters(model, tokenizer, batch_size, seq_length)\n",
    "            results.append((batch_size, seq_length, success))\n",
    "            if not success:\n",
    "                break  # Stop increasing seq_length if OOM\n",
    "\n",
    "    # Print Summary of Results\n",
    "    print(\"\\nTest Results:\")\n",
    "    for batch_size, seq_length, success in results:\n",
    "        status = \"Pass\" if success else \"Fail (OOM)\"\n",
    "        print(f\"Batch Size: {batch_size}, Sequence Length: {seq_length} => {status}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f299c080-3d23-4edc-bde6-8af2109967ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec72ea195fc149b6998fa4e7a1c83832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 1 concurrent users...\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 47, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 46, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 45, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 55, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 40, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 40, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 55, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 50, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 51, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 50, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 55, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 46, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 55, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 47, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 55, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 51, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 55, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 34, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 55, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 49, 81559, 31177\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27562.87 MB\n",
      "GPU Utilization: 54, 81559, 31177\n",
      "Average Response Time (1 users): 1.05s\n",
      "Running Meditron-7B with 2 concurrent users...\n",
      "Memory Allocated: 14073.09 MB\n",
      "Memory Reserved: 27596.42 MB\n",
      "Memory Allocated: 14047.56 MB\n",
      "Memory Reserved: 27596.42 MB\n",
      "GPU Utilization: 53, 81559, 31209\n",
      "GPU Utilization: 53, 81559, 31209\n",
      "Memory Allocated: 14071.82 MB\n",
      "Memory Reserved: 27596.42 MB\n",
      "GPU Utilization: 57, 81559, 31209\n",
      "Memory Allocated: 14051.67 MB\n",
      "Memory Reserved: 27596.42 MB\n",
      "GPU Utilization: 45, 81559, 31209\n",
      "Memory Allocated: 14072.90 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "Memory Allocated: 14047.56 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 56, 81559, 31211\n",
      "GPU Utilization: 56, 81559, 31211\n",
      "Memory Allocated: 14072.93 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "Memory Allocated: 14047.56 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 46, 81559, 31211\n",
      "GPU Utilization: 46, 81559, 31211\n",
      "Memory Allocated: 14071.11 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 57, 81559, 31211\n",
      "Memory Allocated: 14052.32 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 46, 81559, 31211\n",
      "Memory Allocated: 14070.71 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 51, 81559, 31211\n",
      "Memory Allocated: 14052.77 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 51, 81559, 31211\n",
      "Memory Allocated: 14070.49 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 55, 81559, 31211\n",
      "Memory Allocated: 14053.98 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 48, 81559, 31211\n",
      "Memory Allocated: 14069.37 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 46, 81559, 31211\n",
      "Memory Allocated: 14053.83 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 54, 81559, 31211\n",
      "Memory Allocated: 14070.64 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 57, 81559, 31211\n",
      "Memory Allocated: 14052.84 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 46, 81559, 31211\n",
      "Memory Allocated: 14070.73 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 54, 81559, 31211\n",
      "Memory Allocated: 14053.71 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 46, 81559, 31211\n",
      "Memory Allocated: 14069.83 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 57, 81559, 31211\n",
      "Memory Allocated: 14053.52 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 46, 81559, 31211\n",
      "Memory Allocated: 14071.12 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 48, 81559, 31211\n",
      "Memory Allocated: 14052.47 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 48, 81559, 31211\n",
      "Memory Allocated: 14071.39 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 58, 81559, 31211\n",
      "Memory Allocated: 14053.15 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 47, 81559, 31211\n",
      "Memory Allocated: 14070.64 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 54, 81559, 31211\n",
      "Memory Allocated: 14052.86 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 46, 81559, 31211\n",
      "Memory Allocated: 14071.73 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 48, 81559, 31211\n",
      "Memory Allocated: 14047.55 MB\n",
      "Memory Reserved: 27598.52 MB\n",
      "GPU Utilization: 48, 81559, 31211\n",
      "Average Response Time (2 users): 2.03s\n",
      "Running Meditron-7B with 4 concurrent users...\n",
      "Memory Allocated: 14122.17 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 52, 81559, 31269\n",
      "Memory Allocated: 14103.08 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "Memory Allocated: 14077.45 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 45, 81559, 31269\n",
      "Memory Allocated: 14052.08 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 45, 81559, 31269\n",
      "GPU Utilization: 45, 81559, 31269\n",
      "Memory Allocated: 14122.19 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 52, 81559, 31269\n",
      "Memory Allocated: 14098.55 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 35, 81559, 31269\n",
      "Memory Allocated: 14082.31 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "Memory Allocated: 14057.12 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 45, 81559, 31269\n",
      "GPU Utilization: 45, 81559, 31269\n",
      "Memory Allocated: 14119.82 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 45, 81559, 31269\n",
      "Memory Allocated: 14102.79 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 51, 81559, 31269\n",
      "Memory Allocated: 14079.18 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "Memory Allocated: 14055.22 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 39, 81559, 31269\n",
      "GPU Utilization: 39, 81559, 31269\n",
      "Memory Allocated: 14121.55 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 52, 81559, 31269\n",
      "Memory Allocated: 14098.23 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 42, 81559, 31269\n",
      "Memory Allocated: 14080.16 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 38, 81559, 31269\n",
      "Memory Allocated: 14061.68 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 32, 81559, 31269\n",
      "Memory Allocated: 14121.87 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "Memory Allocated: 14096.37 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 41, 81559, 31269\n",
      "GPU Utilization: 41, 81559, 31269\n",
      "Memory Allocated: 14082.10 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 45, 81559, 31269\n",
      "Memory Allocated: 14061.45 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 38, 81559, 31269\n",
      "Memory Allocated: 14119.23 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 36, 81559, 31269\n",
      "Memory Allocated: 14101.87 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 52, 81559, 31269\n",
      "Memory Allocated: 14080.92 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 40, 81559, 31269\n",
      "Memory Allocated: 14058.04 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 40, 81559, 31269\n",
      "Memory Allocated: 14120.85 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 50, 81559, 31269\n",
      "Memory Allocated: 14101.37 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 44, 81559, 31269\n",
      "Memory Allocated: 14080.24 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 37, 81559, 31269\n",
      "Memory Allocated: 14058.32 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 37, 81559, 31269\n",
      "Memory Allocated: 14072.93 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "Memory Allocated: 14047.56 MB\n",
      "Memory Reserved: 27659.34 MB\n",
      "GPU Utilization: 51, 81559, 31269\n",
      "GPU Utilization: 51, 81559, 31269\n",
      "Average Response Time (4 users): 4.53s\n",
      "Running Meditron-7B with 8 concurrent users...\n",
      "Memory Allocated: 14221.54 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 39, 81559, 31395\n",
      "Memory Allocated: 14200.03 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 39, 81559, 31395\n",
      "Memory Allocated: 14185.98 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "Memory Allocated: 14160.57 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 31, 81559, 31395\n",
      "Memory Allocated: 14135.24 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 31, 81559, 31395\n",
      "Memory Allocated: 14110.25 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 31, 81559, 31395\n",
      "Memory Allocated: 14085.30 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 12, 81559, 31395\n",
      "Memory Allocated: 14062.89 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 12, 81559, 31395\n",
      "GPU Utilization: 12, 81559, 31395\n",
      "Memory Allocated: 14218.48 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 41, 81559, 31395\n",
      "Memory Allocated: 14200.88 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 49, 81559, 31395\n",
      "Memory Allocated: 14181.63 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 37, 81559, 31395\n",
      "Memory Allocated: 14160.63 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 31, 81559, 31395\n",
      "Memory Allocated: 14140.38 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 36, 81559, 31395\n",
      "Memory Allocated: 14116.65 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "Memory Allocated: 14091.81 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 24, 81559, 31395\n",
      "Memory Allocated: 14068.54 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 24, 81559, 31395\n",
      "GPU Utilization: 24, 81559, 31395\n",
      "Memory Allocated: 14216.91 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 34, 81559, 31395\n",
      "Memory Allocated: 14200.27 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 25, 81559, 31395\n",
      "Memory Allocated: 14181.00 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 32, 81559, 31395\n",
      "Memory Allocated: 14161.46 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "Memory Allocated: 14136.38 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 25, 81559, 31395\n",
      "GPU Utilization: 25, 81559, 31395\n",
      "Memory Allocated: 14120.49 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 27, 81559, 31395\n",
      "Memory Allocated: 14098.36 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 27, 81559, 31395\n",
      "Memory Allocated: 14077.12 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 28, 81559, 31395\n",
      "Memory Allocated: 14171.09 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 45, 81559, 31395\n",
      "Memory Allocated: 14147.18 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 34, 81559, 31395\n",
      "Memory Allocated: 14122.64 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 34, 81559, 31395\n",
      "Memory Allocated: 14098.23 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 37, 81559, 31395\n",
      "Memory Allocated: 14062.41 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "Memory Allocated: 14047.56 MB\n",
      "Memory Reserved: 27791.46 MB\n",
      "GPU Utilization: 37, 81559, 31395\n",
      "GPU Utilization: 37, 81559, 31395\n",
      "Average Response Time (8 users): 10.55s\n",
      "Running Meditron-7B with 16 concurrent users...\n",
      "Memory Allocated: 14421.30 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 37, 81559, 31641\n",
      "Memory Allocated: 14400.31 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 38, 81559, 31641\n",
      "Memory Allocated: 14377.06 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 30, 81559, 31641\n",
      "Memory Allocated: 14353.48 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14328.56 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 17, 81559, 31641\n",
      "GPU Utilization: 17, 81559, 31641\n",
      "Memory Allocated: 14324.63 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14299.45 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 39, 81559, 31641\n",
      "GPU Utilization: 39, 81559, 31641\n",
      "Memory Allocated: 14277.58 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 21, 81559, 31641\n",
      "Memory Allocated: 14257.90 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14233.55 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 28, 81559, 31641\n",
      "GPU Utilization: 26, 81559, 31641\n",
      "Memory Allocated: 14191.85 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14186.27 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 26, 81559, 31641\n",
      "Memory Allocated: 14118.12 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 18, 81559, 31641\n",
      "Memory Allocated: 14116.41 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14109.54 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 18, 81559, 31641\n",
      "GPU Utilization: 18, 81559, 31641\n",
      "GPU Utilization: 18, 81559, 31641\n",
      "Memory Allocated: 14093.78 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 18, 81559, 31641\n",
      "Memory Allocated: 14366.46 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 45, 81559, 31641\n",
      "Memory Allocated: 14342.04 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 36, 81559, 31641\n",
      "Memory Allocated: 14321.31 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 37, 81559, 31641\n",
      "Memory Allocated: 14299.18 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14273.53 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 44, 81559, 31641\n",
      "GPU Utilization: 44, 81559, 31641\n",
      "Memory Allocated: 14249.96 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14199.52 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14198.31 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 31, 81559, 31641\n",
      "GPU Utilization: 31, 81559, 31641\n",
      "GPU Utilization: 31, 81559, 31641\n",
      "Memory Allocated: 14175.71 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14149.61 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 30, 81559, 31641\n",
      "GPU Utilization: 30, 81559, 31641\n",
      "Memory Allocated: 14124.73 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14075.44 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "Memory Allocated: 14073.42 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 30, 81559, 31641\n",
      "Memory Allocated: 14047.56 MB\n",
      "Memory Reserved: 28049.41 MB\n",
      "GPU Utilization: 30, 81559, 31641\n",
      "GPU Utilization: 15, 81559, 31641\n",
      "GPU Utilization: 15, 81559, 31641\n",
      "Average Response Time (16 users): 21.48s\n",
      "Results saved to results_meditron_7b.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import subprocess\n",
    "\n",
    "# Environment variable to suppress tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"Meditron-7B\"\n",
    "MODEL_PATH = \"epfl-llm/meditron-7b\"\n",
    "TOKEN = \"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\"\n",
    "\n",
    "# Preload tokenizer and model\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    token=TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# Fix: Set pad_token to eos_token if not already defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Function to log GPU utilization using nvidia-smi\n",
    "def log_gpu_utilization():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.total,memory.used\", \"--format=csv,nounits,noheader\"],\n",
    "            stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        utilization = output.decode(\"utf-8\").strip().split(\"\\n\")[0]\n",
    "        print(f\"GPU Utilization: {utilization}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve GPU utilization: {e}\")\n",
    "\n",
    "# Function to process a single query\n",
    "def process_query(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    # Log GPU Memory Usage\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\n",
    "    print(f\"Memory Reserved: {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\n",
    "    log_gpu_utilization()\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True), duration\n",
    "\n",
    "# Function to simulate concurrent users\n",
    "def run_trials(queries, concurrent_users=1):\n",
    "    print(f\"Running {MODEL_NAME} with {concurrent_users} concurrent users...\")\n",
    "    times = []\n",
    "\n",
    "    def run_single_query(query):\n",
    "        _, duration = process_query(query)\n",
    "        return duration\n",
    "\n",
    "    # Run queries in parallel\n",
    "    with ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "        results = executor.map(run_single_query, queries)\n",
    "        times.extend(results)\n",
    "\n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"Average Response Time ({concurrent_users} users): {avg_time:.2f}s\")\n",
    "    return avg_time\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    queries = [\"What is pneumonia?\", \"Explain flu symptoms.\", \"How to treat COVID-19?\"] * 10  # Test load\n",
    "    results = {}\n",
    "\n",
    "    for load in [1, 2, 4, 8, 16]:\n",
    "        avg_time = run_trials(queries, concurrent_users=load)\n",
    "        results[load] = avg_time\n",
    "\n",
    "    # Save results\n",
    "    with open(\"results_meditron_7b.txt\", \"w\") as f:\n",
    "        f.write(str(results))\n",
    "    print(\"Results saved to results_meditron_7b.txt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab90ce-ac72-45c9-91ce-8892c630621a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
