{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae620cda-ae0b-44f0-8d52-289e874ae1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.19.1+cu118)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.8/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.8/dist-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.8/dist-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.8/dist-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.8/dist-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.8/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.8/dist-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.8/dist-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1866aa2-d537-412d-ae26-f39e8e7334e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Device: NVIDIA H100 PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6156a99f-5cf5-4575-b2ea-3388235c5bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (22.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (2.4.1+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e0a16b-772e-4770-918a-1f731d661b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.8/dist-packages (0.45.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from bitsandbytes) (2.4.1+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bitsandbytes) (1.22.2)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from bitsandbytes) (4.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (11.8.86)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.8/dist-packages (from torch->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->bitsandbytes) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be518f07-68eb-4568-bee1-8414320abe68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264dd97f9ee5442f94c95fcfdd4a96ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: What is pneumonia?\n",
      "Pneumonia is an infection of the lungs. It is caused by bacteria, viruses, or fungi. The infection can spread to the lungs through the air or through the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Hugging Face repo for Meditron-7b\n",
    "model_name = \"epfl-llm/meditron-7b\"\n",
    "token = \"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\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=token,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Example input\n",
    "input_text = \"What is pneumonia?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=50)\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Response:\", response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44858eb-56df-4c5a-bcf0-cefd775f5677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601379c49a3d40ff94a750039776205b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: What is pneumonia?\n",
      "Pneumonia is an infection of the lungs. It is caused by bacteria, viruses, or fungi. The infection can spread to the lungs through the air or through the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Hugging Face repo for Meditron-7b\n",
    "model_name = \"epfl-llm/meditron-7b\"\n",
    "token = \"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\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=token,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Example input\n",
    "input_text = \"What is pneumonia?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=50)\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e6a879-dc85-4c92-a9cb-9ca9c24babff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fbb55987a44ea3be32ceb6c4c19efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=token,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"  # Spreads the model across multiple GPUs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11cb9df1-7f82-4daa-9eaf-41083ece80f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Details:\n",
      "Number of GPUs: 1\n",
      "Current GPU: NVIDIA H100 PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available. Details:\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is NOT available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9706ed03-bae5-49e8-9236-cf2e65c88846",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on GPU: tensor([1., 2., 3.], device='cuda:0')\n",
      "Result of GPU computation: tensor([2., 4., 6.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor and move it to GPU\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0]).cuda()\n",
    "print(\"Tensor on GPU:\", tensor)\n",
    "\n",
    "# Perform a calculation on the GPU\n",
    "result = tensor * 2\n",
    "print(\"Result of GPU computation:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59467fe7-5f01-4f0d-94f0-5797656ceaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_query(model, tokenizer, query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            pad_token_id=tokenizer.eos_token_id  # Explicitly set pad_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response, end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e2aa49-c68f-4cc8-b744-df136f8abd26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODELS[model_name], token=HF_TOKEN)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad_token_id globally\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODELS[model_name],\n",
    "        token=HF_TOKEN,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eea6491c-f03d-470a-b110-e0860f2093c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define model name and token\n",
    "model_name = \"epfl-llm/meditron-7b\"\n",
    "token = \"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\"  # Replace with your actual Hugging Face token\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "\n",
    "# Fix the pad_token_id issue\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d740943-d184-44c2-a94a-e278ae79e94c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d25bce472c4cf4ad3bb4f9fe9dd985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 1 concurrent users...\n",
      "Average Response Time (1 users): 1.11s\n",
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7b56bbdd7f4610877560583a916a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 2 concurrent users...\n",
      "Average Response Time (2 users): 2.34s\n",
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9647e40c33745519dd2e035e94b9bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 4 concurrent users...\n",
      "Average Response Time (4 users): 5.71s\n",
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc43479a526e4a978135c1f87efe4a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 8 concurrent users...\n",
      "Average Response Time (8 users): 11.63s\n",
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e79e85c1b24ff1ad46bd1ff50b82d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 16 concurrent users...\n",
      "Average Response Time (16 users): 25.43s\n",
      "Results saved to results_meditron_7b.txt.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"Meditron-7B\"\n",
    "MODEL_PATH = \"epfl-llm/meditron-7b\"\n",
    "TOKEN = \"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\"\n",
    "\n",
    "# Load the model\n",
    "def load_model():\n",
    "    print(f\"Loading {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=TOKEN)\n",
    "    max_memory = {0: \"40GiB\"}  # Allocate memory explicitly for GPU 0\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        token=TOKEN,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"cuda:0\",  # Explicitly set to GPU 0\n",
    "        max_memory=max_memory\n",
    "    )\n",
    "    # Fix: Set pad_token to eos_token if it is not already defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def process_query(model, tokenizer, query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            pad_token_id=tokenizer.pad_token_id  # Explicitly set pad_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True), duration\n",
    "\n",
    "\n",
    "# Function to simulate concurrent users\n",
    "def run_trials(queries, concurrent_users=1):\n",
    "    tokenizer, model = load_model()\n",
    "    print(f\"Running {MODEL_NAME} with {concurrent_users} concurrent users...\")\n",
    "\n",
    "    def run_single_query(query):\n",
    "        _, duration = process_query(model, tokenizer, query)\n",
    "        return duration\n",
    "\n",
    "    times = []\n",
    "    with ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "        results = executor.map(run_single_query, queries)\n",
    "        times.extend(results)\n",
    "\n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"Average Response Time ({concurrent_users} users): {avg_time:.2f}s\")\n",
    "    return avg_time\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    queries = [\"What is pneumonia?\", \"Explain flu symptoms.\", \"How to treat COVID-19?\"] * 10  # Test load\n",
    "    results = {}\n",
    "\n",
    "    for load in [1, 2, 4, 8, 16]:\n",
    "        avg_time = run_trials(queries, concurrent_users=load)\n",
    "        results[load] = avg_time\n",
    "\n",
    "    # Save results\n",
    "    with open(\"results_meditron_7b.txt\", \"w\") as f:\n",
    "        f.write(str(results))\n",
    "    print(\"Results saved to results_meditron_7b.txt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3563badf-e685-4f41-909a-ea8376dbc87c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Meditron-70B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27298b3ac0fd4b3aa8e9555c0e36e34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-70B with 1 concurrent users...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 54\u001b[0m, in \u001b[0;36mrun_trials\u001b[0;34m(queries, concurrent_users)\u001b[0m\n\u001b[1;32m     53\u001b[0m     results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(run_single_query, queries)\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mtimes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m avg_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(times) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(times)\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to results_meditron_70b.txt.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m##################################### Next step ####################################\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 67\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m load \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m]:\n\u001b[0;32m---> 67\u001b[0m     avg_time \u001b[38;5;241m=\u001b[39m \u001b[43mrun_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrent_users\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     results[load] \u001b[38;5;241m=\u001b[39m avg_time\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 54\u001b[0m, in \u001b[0;36mrun_trials\u001b[0;34m(queries, concurrent_users)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mconcurrent_users) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     53\u001b[0m     results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(run_single_query, queries)\n\u001b[0;32m---> 54\u001b[0m     times\u001b[38;5;241m.\u001b[39mextend(results)\n\u001b[1;32m     56\u001b[0m avg_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(times) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(times)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Response Time (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconcurrent_users\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m users): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:644\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/thread.py:236\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 236\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"Meditron-70B\"\n",
    "MODEL_PATH = \"epfl-llm/meditron-70b\"\n",
    "TOKEN = \"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\"\n",
    "\n",
    "# Load the model\n",
    "def load_model():\n",
    "    print(f\"Loading {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=TOKEN)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        token=TOKEN,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    # Fix: Set pad_token to eos_token if it is not already defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# Function to process a single query\n",
    "def process_query(model, tokenizer, query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            pad_token_id=tokenizer.pad_token_id  # Explicitly set pad_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True), duration\n",
    "\n",
    "\n",
    "# Function to simulate concurrent users\n",
    "def run_trials(queries, concurrent_users=1):\n",
    "    tokenizer, model = load_model()\n",
    "    print(f\"Running {MODEL_NAME} with {concurrent_users} concurrent users...\")\n",
    "\n",
    "    def run_single_query(query):\n",
    "        _, duration = process_query(model, tokenizer, query)\n",
    "        return duration\n",
    "\n",
    "    times = []\n",
    "    with ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "        results = executor.map(run_single_query, queries)\n",
    "        times.extend(results)\n",
    "\n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"Average Response Time ({concurrent_users} users): {avg_time:.2f}s\")\n",
    "    return avg_time\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    queries = [\"What is pneumonia?\", \"Explain flu symptoms.\", \"How to treat COVID-19?\"] * 10  # Test load\n",
    "    results = {}\n",
    "\n",
    "    for load in [1, 2, 4, 8, 16]:\n",
    "        avg_time = run_trials(queries, concurrent_users=load)\n",
    "        results[load] = avg_time\n",
    "\n",
    "    # Save results\n",
    "    with open(\"results_meditron_70b.txt\", \"w\") as f:\n",
    "        f.write(str(results))\n",
    "    print(\"Results saved to results_meditron_70b.txt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "##################################### Next step ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c43b7e8-a5b7-4278-a3b5-5dfa67e13659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 1\n",
      "GPU 0: NVIDIA H100 PCIe\n",
      "Tensor successfully created on GPU 0: tensor([-1.2764], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpus():\n",
    "    # Check total number of GPUs\n",
    "    if torch.cuda.is_available():\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs available: {num_gpus}\")\n",
    "        \n",
    "        # Loop through each GPU and print its details\n",
    "        for i in range(num_gpus):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            print(f\"GPU {i}: {gpu_name}\")\n",
    "        \n",
    "        # Verify if tensors can be sent to all GPUs\n",
    "        try:\n",
    "            for i in range(num_gpus):\n",
    "                device = torch.device(f\"cuda:{i}\")\n",
    "                tensor = torch.randn(1).to(device)  # Create a tensor on each GPU\n",
    "                print(f\"Tensor successfully created on GPU {i}: {tensor}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error when accessing GPU {i}: {e}\")\n",
    "    else:\n",
    "        print(\"No GPU detected. Using CPU.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cf19a6f-8e2c-4757-b2f3-6fe7ac172975",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea8f6aa9a4c4a2fbbc100ee38e3978b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Batch Size: 1, Sequence Length: 128...\n",
      "Success: Batch Size 1, Sequence Length 128\n",
      "Testing Batch Size: 1, Sequence Length: 256...\n",
      "Success: Batch Size 1, Sequence Length 256\n",
      "Testing Batch Size: 1, Sequence Length: 512...\n",
      "Success: Batch Size 1, Sequence Length 512\n",
      "Testing Batch Size: 1, Sequence Length: 1024...\n",
      "Success: Batch Size 1, Sequence Length 1024\n",
      "Testing Batch Size: 1, Sequence Length: 2048...\n",
      "Success: Batch Size 1, Sequence Length 2048\n",
      "Testing Batch Size: 2, Sequence Length: 128...\n",
      "Success: Batch Size 2, Sequence Length 128\n",
      "Testing Batch Size: 2, Sequence Length: 256...\n",
      "Success: Batch Size 2, Sequence Length 256\n",
      "Testing Batch Size: 2, Sequence Length: 512...\n",
      "Success: Batch Size 2, Sequence Length 512\n",
      "Testing Batch Size: 2, Sequence Length: 1024...\n",
      "Success: Batch Size 2, Sequence Length 1024\n",
      "Testing Batch Size: 2, Sequence Length: 2048...\n",
      "Success: Batch Size 2, Sequence Length 2048\n",
      "Testing Batch Size: 4, Sequence Length: 128...\n",
      "Success: Batch Size 4, Sequence Length 128\n",
      "Testing Batch Size: 4, Sequence Length: 256...\n",
      "Success: Batch Size 4, Sequence Length 256\n",
      "Testing Batch Size: 4, Sequence Length: 512...\n",
      "Success: Batch Size 4, Sequence Length 512\n",
      "Testing Batch Size: 4, Sequence Length: 1024...\n",
      "Success: Batch Size 4, Sequence Length 1024\n",
      "Testing Batch Size: 4, Sequence Length: 2048...\n",
      "Success: Batch Size 4, Sequence Length 2048\n",
      "Testing Batch Size: 8, Sequence Length: 128...\n",
      "Success: Batch Size 8, Sequence Length 128\n",
      "Testing Batch Size: 8, Sequence Length: 256...\n",
      "Success: Batch Size 8, Sequence Length 256\n",
      "Testing Batch Size: 8, Sequence Length: 512...\n",
      "Success: Batch Size 8, Sequence Length 512\n",
      "Testing Batch Size: 8, Sequence Length: 1024...\n",
      "Success: Batch Size 8, Sequence Length 1024\n",
      "Testing Batch Size: 8, Sequence Length: 2048...\n",
      "Success: Batch Size 8, Sequence Length 2048\n",
      "Testing Batch Size: 16, Sequence Length: 128...\n",
      "Success: Batch Size 16, Sequence Length 128\n",
      "Testing Batch Size: 16, Sequence Length: 256...\n",
      "Success: Batch Size 16, Sequence Length 256\n",
      "Testing Batch Size: 16, Sequence Length: 512...\n",
      "Success: Batch Size 16, Sequence Length 512\n",
      "Testing Batch Size: 16, Sequence Length: 1024...\n",
      "Success: Batch Size 16, Sequence Length 1024\n",
      "Testing Batch Size: 16, Sequence Length: 2048...\n",
      "OOM Error: Batch Size 16, Sequence Length 2048\n",
      "\n",
      "Test Results:\n",
      "Batch Size: 1, Sequence Length: 128 => Pass\n",
      "Batch Size: 1, Sequence Length: 256 => Pass\n",
      "Batch Size: 1, Sequence Length: 512 => Pass\n",
      "Batch Size: 1, Sequence Length: 1024 => Pass\n",
      "Batch Size: 1, Sequence Length: 2048 => Pass\n",
      "Batch Size: 2, Sequence Length: 128 => Pass\n",
      "Batch Size: 2, Sequence Length: 256 => Pass\n",
      "Batch Size: 2, Sequence Length: 512 => Pass\n",
      "Batch Size: 2, Sequence Length: 1024 => Pass\n",
      "Batch Size: 2, Sequence Length: 2048 => Pass\n",
      "Batch Size: 4, Sequence Length: 128 => Pass\n",
      "Batch Size: 4, Sequence Length: 256 => Pass\n",
      "Batch Size: 4, Sequence Length: 512 => Pass\n",
      "Batch Size: 4, Sequence Length: 1024 => Pass\n",
      "Batch Size: 4, Sequence Length: 2048 => Pass\n",
      "Batch Size: 8, Sequence Length: 128 => Pass\n",
      "Batch Size: 8, Sequence Length: 256 => Pass\n",
      "Batch Size: 8, Sequence Length: 512 => Pass\n",
      "Batch Size: 8, Sequence Length: 1024 => Pass\n",
      "Batch Size: 8, Sequence Length: 2048 => Pass\n",
      "Batch Size: 16, Sequence Length: 128 => Pass\n",
      "Batch Size: 16, Sequence Length: 256 => Pass\n",
      "Batch Size: 16, Sequence Length: 512 => Pass\n",
      "Batch Size: 16, Sequence Length: 1024 => Pass\n",
      "Batch Size: 16, Sequence Length: 2048 => Fail (OOM)\n"
     ]
    }
   ],
   "source": [
    "# BATCH TEST #\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Model and Token Configuration\n",
    "MODEL_PATH = \"epfl-llm/meditron-7b\"\n",
    "TOKEN = \"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\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "def load_model():\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=TOKEN)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        token=TOKEN,\n",
    "        torch_dtype=torch.float16,  # Mixed precision\n",
    "        device_map=\"auto\"           # Automatically allocate to GPU\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to Test Memory for Given Batch Size and Sequence Length\n",
    "def test_hyperparameters(model, tokenizer, batch_size, seq_length):\n",
    "    try:\n",
    "        print(f\"Testing Batch Size: {batch_size}, Sequence Length: {seq_length}...\")\n",
    "        input_text = \"Test input sentence. \" * (seq_length // 5)  # Simulate input text\n",
    "        inputs = tokenizer(\n",
    "            [input_text] * batch_size,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=seq_length\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # Perform a forward pass with max_new_tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,  # Replace max_length with max_new_tokens\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        print(f\"Success: Batch Size {batch_size}, Sequence Length {seq_length}\")\n",
    "        return True\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"OOM Error: Batch Size {batch_size}, Sequence Length {seq_length}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Main Function to Test Combinations\n",
    "def main():\n",
    "    tokenizer, model = load_model()\n",
    "\n",
    "    # Range of Hyperparameters to Test\n",
    "    batch_sizes = [1, 2, 4, 8, 16]          # Micro batch sizes\n",
    "    sequence_lengths = [128, 256, 512, 1024, 2048]  # Sequence lengths\n",
    "\n",
    "    results = []\n",
    "    for batch_size in batch_sizes:\n",
    "        for seq_length in sequence_lengths:\n",
    "            success = test_hyperparameters(model, tokenizer, batch_size, seq_length)\n",
    "            results.append((batch_size, seq_length, success))\n",
    "            if not success:\n",
    "                break  # Stop increasing seq_length if OOM\n",
    "\n",
    "    # Print Summary of Results\n",
    "    print(\"\\nTest Results:\")\n",
    "    for batch_size, seq_length, success in results:\n",
    "        status = \"Pass\" if success else \"Fail (OOM)\"\n",
    "        print(f\"Batch Size: {batch_size}, Sequence Length: {seq_length} => {status}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f299c080-3d23-4edc-bde6-8af2109967ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Meditron-7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd5146125c648dfa0bff9a78c253a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Meditron-7B with 1 concurrent users...\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 49, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 56, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 44, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 56, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 56, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 54, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 51, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 56, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 37, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 41, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 54, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 43, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 56, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 56, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 47, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 56, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 44, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 56, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Memory Allocated: 13510.68 MB\n",
      "Memory Reserved: 13545.50 MB\n",
      "GPU Utilization: 55, 81559, 27417\n",
      "Average Response Time (1 users): 1.04s\n",
      "Running Meditron-7B with 2 concurrent users...\n",
      "Memory Allocated: 13548.25 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "Memory Allocated: 13544.24 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 42, 81559, 27737\n",
      "GPU Utilization: 42, 81559, 27737\n",
      "Memory Allocated: 13569.32 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 50, 81559, 27737\n",
      "Memory Allocated: 13544.65 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 50, 81559, 27737\n",
      "Memory Allocated: 13570.05 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "Memory Allocated: 13544.24 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 50, 81559, 27737\n",
      "GPU Utilization: 50, 81559, 27737\n",
      "Memory Allocated: 13569.04 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 57, 81559, 27737\n",
      "Memory Allocated: 13546.75 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 38, 81559, 27737\n",
      "Memory Allocated: 13568.60 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 56, 81559, 27737\n",
      "Memory Allocated: 13548.63 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 45, 81559, 27737\n",
      "Memory Allocated: 13569.36 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "Memory Allocated: 13544.24 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 45, 81559, 27737\n",
      "GPU Utilization: 45, 81559, 27737\n",
      "Memory Allocated: 13569.47 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "Memory Allocated: 13544.24 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 55, 81559, 27737\n",
      "GPU Utilization: 55, 81559, 27737\n",
      "Memory Allocated: 13567.94 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 45, 81559, 27737\n",
      "Memory Allocated: 13548.82 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 47, 81559, 27737\n",
      "Memory Allocated: 13568.46 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 55, 81559, 27737\n",
      "Memory Allocated: 13548.33 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 39, 81559, 27737\n",
      "Memory Allocated: 13568.89 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "Memory Allocated: 13544.24 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 57, 81559, 27737\n",
      "GPU Utilization: 57, 81559, 27737\n",
      "Memory Allocated: 13568.48 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 55, 81559, 27737\n",
      "Memory Allocated: 13548.38 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 42, 81559, 27737\n",
      "Memory Allocated: 13569.48 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "Memory Allocated: 13544.24 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 60, 81559, 27737\n",
      "GPU Utilization: 60, 81559, 27737\n",
      "Memory Allocated: 13568.70 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 59, 81559, 27737\n",
      "Memory Allocated: 13549.53 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 40, 81559, 27737\n",
      "Memory Allocated: 13567.67 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 57, 81559, 27737\n",
      "Memory Allocated: 13549.19 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 47, 81559, 27737\n",
      "Memory Allocated: 13568.49 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 57, 81559, 27737\n",
      "Memory Allocated: 13544.24 MB\n",
      "Memory Reserved: 13612.61 MB\n",
      "GPU Utilization: 38, 81559, 27737\n",
      "Average Response Time (2 users): 1.96s\n",
      "Running Meditron-7B with 4 concurrent users...\n",
      "Memory Allocated: 13686.15 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 44, 81559, 28375\n",
      "Memory Allocated: 13642.29 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "Memory Allocated: 13641.11 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "Memory Allocated: 13615.37 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 37, 81559, 28375\n",
      "GPU Utilization: 37, 81559, 28375\n",
      "GPU Utilization: 37, 81559, 28375\n",
      "Memory Allocated: 13686.07 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "Memory Allocated: 13661.37 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 44, 81559, 28375\n",
      "GPU Utilization: 44, 81559, 28375\n",
      "Memory Allocated: 13645.72 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 36, 81559, 28375\n",
      "Memory Allocated: 13623.03 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 27, 81559, 28375\n",
      "Memory Allocated: 13683.33 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 46, 81559, 28375\n",
      "Memory Allocated: 13664.60 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 44, 81559, 28375\n",
      "Memory Allocated: 13645.96 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "Memory Allocated: 13621.38 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 46, 81559, 28375\n",
      "GPU Utilization: 46, 81559, 28375\n",
      "Memory Allocated: 13686.87 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "Memory Allocated: 13661.42 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 50, 81559, 28375\n",
      "GPU Utilization: 50, 81559, 28375\n",
      "Memory Allocated: 13643.68 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 39, 81559, 28375\n",
      "Memory Allocated: 13621.85 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 37, 81559, 28375\n",
      "Memory Allocated: 13685.34 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 46, 81559, 28375\n",
      "Memory Allocated: 13662.03 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 46, 81559, 28375\n",
      "Memory Allocated: 13645.07 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 42, 81559, 28375\n",
      "Memory Allocated: 13624.64 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 31, 81559, 28375\n",
      "Memory Allocated: 13684.84 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 51, 81559, 28375\n",
      "Memory Allocated: 13665.87 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 44, 81559, 28375\n",
      "Memory Allocated: 13642.34 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 38, 81559, 28375\n",
      "Memory Allocated: 13620.67 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 38, 81559, 28375\n",
      "Memory Allocated: 13686.30 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 45, 81559, 28375\n",
      "Memory Allocated: 13661.35 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 45, 81559, 28375\n",
      "Memory Allocated: 13642.69 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 38, 81559, 28375\n",
      "Memory Allocated: 13621.38 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 38, 81559, 28375\n",
      "Memory Allocated: 13636.34 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 49, 81559, 28375\n",
      "Memory Allocated: 13611.35 MB\n",
      "Memory Reserved: 13744.73 MB\n",
      "GPU Utilization: 49, 81559, 28375\n",
      "Average Response Time (4 users): 4.51s\n",
      "Running Meditron-7B with 8 concurrent users...\n",
      "Memory Allocated: 13920.24 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "Memory Allocated: 13894.63 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 47, 81559, 29653\n",
      "GPU Utilization: 32, 81559, 29653\n",
      "Memory Allocated: 13884.11 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "Memory Allocated: 13858.54 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 45, 81559, 29653\n",
      "Memory Allocated: 13832.88 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 45, 81559, 29653\n",
      "Memory Allocated: 13808.10 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 45, 81559, 29653\n",
      "Memory Allocated: 13783.39 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 13, 81559, 29653\n",
      "Memory Allocated: 13758.48 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 13, 81559, 29653\n",
      "GPU Utilization: 13, 81559, 29653\n",
      "Memory Allocated: 13914.86 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 46, 81559, 29653\n",
      "Memory Allocated: 13900.43 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "Memory Allocated: 13875.17 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 39, 81559, 29653\n",
      "Memory Allocated: 13849.92 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 39, 81559, 29653\n",
      "GPU Utilization: 39, 81559, 29653\n",
      "Memory Allocated: 13841.46 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "Memory Allocated: 13815.95 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 34, 81559, 29653\n",
      "GPU Utilization: 34, 81559, 29653\n",
      "Memory Allocated: 13794.39 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 24, 81559, 29653\n",
      "Memory Allocated: 13772.11 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 24, 81559, 29653\n",
      "Memory Allocated: 13916.77 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 46, 81559, 29653\n",
      "Memory Allocated: 13893.73 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 37, 81559, 29653\n",
      "Memory Allocated: 13880.45 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "Memory Allocated: 13855.04 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 36, 81559, 29653\n",
      "GPU Utilization: 36, 81559, 29653\n",
      "Memory Allocated: 13834.22 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "Memory Allocated: 13808.98 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 23, 81559, 29653\n",
      "GPU Utilization: 23, 81559, 29653\n",
      "Memory Allocated: 13801.95 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "Memory Allocated: 13776.25 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 24, 81559, 29653\n",
      "GPU Utilization: 24, 81559, 29653\n",
      "Memory Allocated: 13871.56 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "Memory Allocated: 13846.17 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 38, 81559, 29653\n",
      "Memory Allocated: 13820.99 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 38, 81559, 29653\n",
      "Memory Allocated: 13795.98 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 38, 81559, 29653\n",
      "GPU Utilization: 23, 81559, 29653\n",
      "Memory Allocated: 13771.41 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "Memory Allocated: 13745.57 MB\n",
      "Memory Reserved: 14011.07 MB\n",
      "GPU Utilization: 23, 81559, 29653\n",
      "GPU Utilization: 23, 81559, 29653\n",
      "Average Response Time (8 users): 9.52s\n",
      "Running Meditron-7B with 16 concurrent users...\n",
      "Memory Allocated: 14390.94 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 34, 81559, 32203\n",
      "Memory Allocated: 14348.84 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "Memory Allocated: 14341.26 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 28, 81559, 32203\n",
      "Memory Allocated: 14317.10 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 28, 81559, 32203\n",
      "GPU Utilization: 16, 81559, 32203\n",
      "Memory Allocated: 14293.36 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 16, 81559, 32203\n",
      "Memory Allocated: 14289.47 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 32, 81559, 32203\n",
      "Memory Allocated: 14265.17 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 25, 81559, 32203\n",
      "Memory Allocated: 14244.93 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "Memory Allocated: 14221.49 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 31, 81559, 32203\n",
      "GPU Utilization: 31, 81559, 32203\n",
      "Memory Allocated: 14201.05 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "Memory Allocated: 14175.01 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 17, 81559, 32203\n",
      "Memory Allocated: 14151.05 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 15, 81559, 32203\n",
      "Memory Allocated: 14126.37 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 15, 81559, 32203\n",
      "GPU Utilization: 15, 81559, 32203\n",
      "Memory Allocated: 14104.65 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 17, 81559, 32203\n",
      "Memory Allocated: 14083.96 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "Memory Allocated: 14059.36 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 20, 81559, 32203\n",
      "GPU Utilization: 20, 81559, 32203\n",
      "Memory Allocated: 14334.57 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 34, 81559, 32203\n",
      "Memory Allocated: 14310.67 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 29, 81559, 32203\n",
      "Memory Allocated: 14290.74 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "Memory Allocated: 14265.24 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 36, 81559, 32203\n",
      "GPU Utilization: 36, 81559, 32203\n",
      "Memory Allocated: 14240.89 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "Memory Allocated: 14215.79 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 24, 81559, 32203\n",
      "GPU Utilization: 24, 81559, 32203\n",
      "Memory Allocated: 14180.31 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "Memory Allocated: 14165.95 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 25, 81559, 32203\n",
      "Memory Allocated: 14140.09 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 25, 81559, 32203\n",
      "GPU Utilization: 25, 81559, 32203\n",
      "Memory Allocated: 14116.83 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "Memory Allocated: 14091.11 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 34, 81559, 32203\n",
      "Memory Allocated: 14065.64 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 34, 81559, 32203\n",
      "Memory Allocated: 14039.83 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "Memory Allocated: 14014.00 MB\n",
      "Memory Reserved: 14535.36 MB\n",
      "GPU Utilization: 18, 81559, 32203\n",
      "GPU Utilization: 18, 81559, 32203\n",
      "GPU Utilization: 18, 81559, 32203\n",
      "Average Response Time (16 users): 25.63s\n",
      "Results saved to results_meditron_7b.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import subprocess\n",
    "\n",
    "# Environment variable to suppress tokenizer parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"Meditron-7B\"\n",
    "MODEL_PATH = \"epfl-llm/meditron-7b\"\n",
    "TOKEN = \"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\"\n",
    "\n",
    "# Preload tokenizer and model\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    token=TOKEN,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# Fix: Set pad_token to eos_token if not already defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Function to log GPU utilization using nvidia-smi\n",
    "def log_gpu_utilization():\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.total,memory.used\", \"--format=csv,nounits,noheader\"],\n",
    "            stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        utilization = output.decode(\"utf-8\").strip().split(\"\\n\")[0]\n",
    "        print(f\"GPU Utilization: {utilization}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve GPU utilization: {e}\")\n",
    "\n",
    "# Function to process a single query\n",
    "def process_query(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    # Log GPU Memory Usage\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\n",
    "    print(f\"Memory Reserved: {torch.cuda.memory_reserved() / 1e6:.2f} MB\")\n",
    "    log_gpu_utilization()\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True), duration\n",
    "\n",
    "# Function to simulate concurrent users\n",
    "def run_trials(queries, concurrent_users=1):\n",
    "    print(f\"Running {MODEL_NAME} with {concurrent_users} concurrent users...\")\n",
    "    times = []\n",
    "\n",
    "    def run_single_query(query):\n",
    "        _, duration = process_query(query)\n",
    "        return duration\n",
    "\n",
    "    # Run queries in parallel\n",
    "    with ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "        results = executor.map(run_single_query, queries)\n",
    "        times.extend(results)\n",
    "\n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"Average Response Time ({concurrent_users} users): {avg_time:.2f}s\")\n",
    "    return avg_time\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    queries = [\"What is pneumonia?\", \"Explain flu symptoms.\", \"How to treat COVID-19?\"] * 10  # Test load\n",
    "    results = {}\n",
    "\n",
    "    for load in [1, 2, 4, 8, 16]:\n",
    "        avg_time = run_trials(queries, concurrent_users=load)\n",
    "        results[load] = avg_time\n",
    "\n",
    "    # Save results\n",
    "    with open(\"results_meditron_7b.txt\", \"w\") as f:\n",
    "        f.write(str(results))\n",
    "    print(\"Results saved to results_meditron_7b.txt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
